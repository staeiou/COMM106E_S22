{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a machine learning text classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas seaborn nltk scikit-learn==1.0.2 transformers tensorflow emoji torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "import sklearn\n",
    "import sklearn.pipeline\n",
    "import sklearn.feature_extraction.text\n",
    "import sklearn.naive_bayes\n",
    "import sklearn.model_selection\n",
    "import sklearn.metrics\n",
    "import tensorflow\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and exploring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"comm106e_happysad.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label'] = data['label'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = sklearn.pipeline.Pipeline([\n",
    "    ('vect', sklearn.feature_extraction.text.CountVectorizer()),\n",
    "    ('tfidf', sklearn.feature_extraction.text.TfidfTransformer()),\n",
    "    ('clf', sklearn.naive_bayes.MultinomialNB()),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(data['text'], data['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring some text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails_to_score = ['I need this report by 9am or else I\\'ll be mad!',\n",
    "                   'I love it, I\\'m so happy!',\n",
    "                   'our next conference should be in France',\n",
    "                   'our next conference should be in Germany'\n",
    "                  ]\n",
    "\n",
    "prediction = pipeline.predict_proba(emails_to_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-training with only 80% of the texts, leaving the other 20% for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into training and testing sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(data['text'], \n",
    "                                                                            data['label'], \n",
    "                                                                            test_size=0.2, \n",
    "                                                                            random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model by fitting the pipeline to the 80% training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the pipeline to predict the labels of the 20% testing data that was not used to train it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score the accuracy of the model with the results of the predictions made from the 20% testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print a classification report for how the 20% test data did, showing various statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(sklearn.metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some definitions\n",
    "\n",
    "But please look at the chart at https://en.wikipedia.org/wiki/Precision_and_recall#Definition_(classification_context)\n",
    "\n",
    "## Accuracy:\n",
    "\n",
    "$\\frac{\\mbox{number of correct predictions}}{\\mbox{total number of items predicted}}$\n",
    "\n",
    "## Precision or positive predictive value (PPV): \n",
    "\n",
    "$\\frac{\\mbox{number of true positives for a label}}{\\mbox{number of true + false positives for that label}}$\n",
    "\n",
    "## True positive rate (TPR) or recall, sensitivity, hit rate:\n",
    "\n",
    "$\\frac{\\mbox{number of true positives for a label}}{\\mbox{number of actual positives for that label}}$\n",
    "\n",
    "## F1 score (harmonic mean between precision/PPV and recall/TPR): \n",
    "\n",
    "$2*\\frac{\\mbox{precision * recall}}{\\mbox{precision + recall}}$\n",
    "\n",
    "## Support: Number of cases in each label or all labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running our classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sklearn.metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.precision_score(y_test, y_pred, pos_label='happy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auditing the model for variations in locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = pd.read_csv(\"countries.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "for country_name in countries['country']:\n",
    "\n",
    "    sample_text = \"It is so nice in \" + country_name + \", I love it there!\"\n",
    "    probability = pipeline.predict_proba([sample_text])[0][0]\n",
    "    \n",
    "    result = {'country':country_name,\n",
    "              'happy_prediction':probability}\n",
    "    \n",
    "    results_list.append(result)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_audit_data = pd.DataFrame(results_list)\n",
    "country_audit_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_audit_data.sort_values('happy_prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "97b4d432cc475b37b45bc9f941b0f61c6e9f18b4be8b57c80de35b509106efdd"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
